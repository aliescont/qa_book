{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bittfmenvconda37cfa90e46c646e49950077966ece666",
   "display_name": "Python 3.6.9 64-bit ('tfmenv': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = pd.read_csv('data_outputs/book_df.csv')\n",
    "book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = book_df['ID'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/SimpleDBpediaQA/train.txt',sep='\\t', header= None)\n",
    "train_df.columns = ['id','Question', 'dbpedia_resource', 'dbpedia_predicate', 'direction', 'dbpedia_transl', 'freebase', 'entity']\n",
    "test_df = pd.read_csv('data/SimpleDBpediaQA/test.txt',sep='\\t', header= None)\n",
    "test_df.columns = ['id','Question', 'dbpedia_resource', 'dbpedia_predicate', 'direction', 'dbpedia_transl', 'freebase', 'entity']\n",
    "valid_df = pd.read_csv('data/SimpleDBpediaQA/valid.txt',sep='\\t', header= None)\n",
    "valid_df.columns = ['id','Question', 'dbpedia_resource', 'dbpedia_predicate', 'direction', 'dbpedia_transl', 'freebase', 'entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df, test_df, valid_df])\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = book_df.merge(full_df[['id','entity']], how = 'left', left_on='ID', right_on='id')\n",
    "df = df.drop_duplicates('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df = df[df['FreebasePredicate']=='books_in_genre']\n",
    "author_df = df[df['FreebasePredicate']=='book_from_author']\n",
    "book_ent_df = df[df['FreebasePredicate']=='author_of_book']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuple(entities, questions, ids):\n",
    "    #genre_df['entity'].str.split(), genre_df['Query'].str.split(), genre_df['ID']\n",
    "    question_tuple = []\n",
    "    result = dict()\n",
    "    for token, q, id_ in zip(entities, questions, ids):\n",
    "        for t, question in zip(token, q):\n",
    "            question = re.sub(r'[^\\w\\s]','',question.lower())\n",
    "            if t == 'I':\n",
    "                question_tuple.append((id_, question))\n",
    "    return question_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_tuple = create_tuple(genre_df['entity'].str.split(), genre_df['Query'].str.split(), genre_df['ID'])\n",
    "book_tuple = create_tuple(book_ent_df['entity'].str.split(), book_ent_df['Query'].str.split(), book_ent_df['ID'])\n",
    "author_tuple = create_tuple(author_df['entity'].str.split(), author_df['Query'].str.split(), author_df['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(tuples):\n",
    "    d = defaultdict(list)\n",
    "    for k, v in tuples:\n",
    "        d[k].append(v)\n",
    "    for k, v in d.items():\n",
    "        d[k] = (' '. join(map(str, v)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = create_dict(genre_tuple)\n",
    "book_dict = create_dict(book_tuple)\n",
    "author_dict = create_dict(author_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train(dict_, questions, ids, label):\n",
    "    ent_list =[]\n",
    "    ent_dict = defaultdict()\n",
    "    train = []\n",
    "    for question, id_ in zip(questions, ids):\n",
    "        question = re.sub(r'[^\\w\\s]','',question.lower())\n",
    "        ent = str(dict_[id_])\n",
    "        if ent != '[]':   \n",
    "            ent_words = re.search(ent, question)\n",
    "            ent_list.append((question, (ent_words.start(), ent_words.end(), label)))  \n",
    "\n",
    "    for v in ent_list:\n",
    "        ent_dict[\"entities\"] = v\n",
    "        \n",
    "    for question, entities in ent_list:\n",
    "        entities = {\"entities\": [entities]}\n",
    "        train.append((question, entities))\n",
    "    return train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_entities = create_train(genre_dict, genre_df['Query'], genre_df['ID'], 'GENRE')\n",
    "book_entities = create_train(book_dict, book_ent_df['Query'], book_ent_df['ID'], 'BOOK')\n",
    "author_entities = create_train(author_dict, author_df['Query'], author_df['ID'], 'PERSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load question generated from abstracts\n",
    "import json\n",
    "\n",
    "with open('/home/aliciescont/Documents/tfm_code/question_generation/abstract.json') as f:\n",
    "    abstract = json.load(f)\n",
    "question_list = []\n",
    "\n",
    "for qg in abstract.values():\n",
    "    for question in qg:\n",
    "        question_list.append(question)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#add date, lang, loc to avoid catastrophic forgetting\n",
    "date_entities = []\n",
    "lang_entities = []\n",
    "loc_entities = []\n",
    "for question in question_list:\n",
    "    question = re.sub(r'[^\\w\\s]',' ',question.lower())\n",
    "    print(question)\n",
    "    if question.isalpha() == True:\n",
    "        doc = nlp(question)\n",
    "    \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'LANGUAGE':\n",
    "                lang_entities.append((question, {'entities' : [ent.text, ent.start, ent.end, ent.label_]}))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  author_entities +  genre_entities + book_entities + lang_entities "
   ]
  },
  {
   "source": [
    "# Custom NER Spacy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe(\"ner\")\n",
    "entity_labels = ['GENRE', 'BOOK']\n",
    "for label in entity_labels:\n",
    "    ner.add_label(label)\n",
    "#disable component not needed\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "optimizer = nlp.resume_training()\n",
    "with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    for itn in range(50):\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=sizes)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "        print(\"Losses\", losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"what are some science fiction\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"who wrote 1984 \")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"what books have stephen king written \")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('/home/aliciescont/Documents/tfm_code/QA_eval/SimpleDBpediaQA/V1')\n",
    "nlp.to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_updated = spacy.load(output_dir)\n",
    "doc= nlp_updated(\"who wrote a book published in english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_names = list(ner.move_names)\n",
    "  \n",
    "assert nlp_updated.get_pipe(\"ner\").move_names == move_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = 'new_model_name'  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp2 = spacy.load(output_dir)\n",
    "        \n",
    "assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"who is the author of 1984\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"what is book written in english\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "source": [
    "https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}