{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bittfmenvconda37cfa90e46c646e49950077966ece666",
   "display_name": "Python 3.6.9 64-bit ('tfmenv': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import re \n",
    "import spotlight\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from random import sample\n",
    "import sparql\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "source": [
    "# SimpleDBpediaQA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "\n",
    "with open('data/SimpleDBpediaQA/train.json') as f:\n",
    "    train = json.load(f)\n",
    "with open('data/SimpleDBpediaQA/test.json') as f:\n",
    "    test = json.load(f)\n",
    "with open('data/SimpleDBpediaQA/valid.json') as f:\n",
    "    valid = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train['Questions'])\n",
    "valid_df = pd.DataFrame(valid['Questions'])\n",
    "test_df = pd.DataFrame(test['Questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all data from SimpleDBpediaQA to filter questions related to books\n",
    "simple_df = pd.concat([train_df, valid_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of conversion from Freebase to DBpedia\n",
    "pred_dict = {}\n",
    "for key, value in zip(simple_df['FreebasePredicate'], simple_df['PredicateList']):\n",
    "    pred_dict[key] = value[0]['Predicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBpedia predicates to be filtered\n",
    "ont_list = ['http://dbpedia.org/ontology/notableWork', 'http://dbpedia.org/ontology/WrittenWork', 'http://dbpedia.org/ontology/author' , 'http://dbpedia.org/ontology/publisher',  'http://dbpedia.org/ontology/subsequentWork', 'http://dbpedia.org/ontology/Country', 'http://dbpedia.org/property/author', 'http://dbpedia.org/ontology/illustrator', 'http://dbpedia.org/property/notableworks>', 'http://dbpedia.org/ontology/nationality', 'http://dbpedia.org/ontology/releaseDate', 'http://dbpedia.org/ontology/birthDate', 'http://dbpedia.org/ontology/birthName', 'http://dbpedia.org/ontology/birthPlace', 'http://dbpedia.org/ontology/occupation', 'http://dbpedia.org/ontology/influencedBy', 'http://dbpedia.org/ontology/literaryGenre', 'http://dbpedia.org/ontology/country','http://dbpedia.org/ontology/notableWork', 'http://dbpedia.org/ontology/previousWork']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of freebase and DBpedia translation filtered\n",
    "for key, value in pred_dict.items():\n",
    "    if value in ont_list:  \n",
    "        print(key + ' --> ' + value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace entity by their label\n",
    "def replace_entity(data_list):\n",
    "    replace_list = []\n",
    "    for question in data_list:\n",
    "        doc = nlp(question)\n",
    "        for ent in doc.ents:\n",
    "          \n",
    "            #ent_label = str(ent.label_)\n",
    "            ent_label = 'ENT'\n",
    "            question = question.replace(question[ent.start_char:ent.end_char], ent_label)\n",
    "            replace_list.append(question)\n",
    "    return replace_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter predicate based on regex exp\n",
    "def regex_filter(exp,val):\n",
    "    if val:\n",
    "        regex = re.search(exp,val)\n",
    "        if regex:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform predicate to a domain filter by the first word after www.freebase.com/\n",
    "simple_df['theme'] = simple_df['FreebasePredicate']\n",
    "simple_df['theme'] = simple_df['theme'].str.replace(\"www.freebase.com/\",\"\")\n",
    "simple_df['theme'] = simple_df['theme'].apply(lambda x : re.sub(r\"(/.*)\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution plot of simple_df by domain\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = simple_df['theme'].value_counts().sort_index(ascending=True)\n",
    "ax.plot(kind='bar')\n",
    "fig.savefig('images/simple_theme.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence length of SimpleDbpediaQA dataset\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.hist([len(question.split()) for question in simple_df['Query'] ], bins= 25)\n",
    "fig.savefig('images/len_question_all.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(question.split()) for question in simple_df['Query'] ])"
   ]
  },
  {
   "source": [
    "## Book questions of SimpleDBpediaQA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicates related to books, but not contain in freebase book domain. Some of them are not directly related to books, but tha amount of available question shouldn't affect the classification\n",
    "pred_book = ['www.freebase.com/media_common/literary_genre/books_in_this_genre', 'www.freebase.com/cvg/computer_videogame/publisher', 'www.freebase.com/film/film/country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select predicates related to freebase domain books\n",
    "book_set = set([pred for pred in simple_df['FreebasePredicate'] if regex_filter(r'^www.freebase.com/book(.*)', pred) == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_list = simple_df['Query'][simple_df['FreebasePredicate'] == 'www.freebase.com/cvg/computer_videogame/publisher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get similarity between a target type of questions and an input question\n",
    "def make_similar(reference, input_q):\n",
    "    q_vec = []\n",
    "    input_vec = nlp(input_q)\n",
    "\n",
    "    for question in reference:\n",
    "        question_vec = nlp(question)\n",
    "        q_vec.append(input_vec.similarity(question_vec))\n",
    "        return max(q_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering predicates relating to newspaper.\n",
    "book_set = ['www.freebase.com/book/written_work/original_language',\n",
    " 'www.freebase.com/book/book/genre',\n",
    " 'www.freebase.com/book/author/works_written',\n",
    " 'www.freebase.com/book/written_work/author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop predicates related to newspapers and add other type of predicates not directely related to book domain, but can be asked\n",
    "book_filter = book_set + pred_book\n",
    "book_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering dataset to the selected predicates\n",
    "train_book = train_df[train_df['FreebasePredicate'].isin(book_filter)]\n",
    "valid_book = valid_df[valid_df['FreebasePredicate'].isin(book_filter)]\n",
    "test_book = test_df[test_df['FreebasePredicate'].isin(book_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge train, valid and test of the subset\n",
    "simple_df_book = pd.concat([train_book, test_book, valid_book])\n",
    "simple_df_book = simple_df_book.reset_index(drop=True)\n",
    "simple_df_book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df_book['FreebasePredicate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use predicate from Freebase to identify intents\n",
    "simple_df_book['FreebasePredicate'].loc[simple_df_book['FreebasePredicate'] == 'www.freebase.com/book/book/genre'] = 'genre'\n",
    "simple_df_book['FreebasePredicate'].loc[simple_df_book['FreebasePredicate'] == 'www.freebase.com/media_common/literary_genre/books_in_this_genre'] = 'books_in_genre'\n",
    "simple_df_book['FreebasePredicate'].loc[simple_df_book['FreebasePredicate'] == 'www.freebase.com/book/written_work/author'] = 'author_of_book'\n",
    "simple_df_book['FreebasePredicate'].loc[simple_df_book['FreebasePredicate'] == 'www.freebase.com/book/author/works_written'] = 'book_from_author'\n",
    "simple_df_book['FreebasePredicate'].loc[simple_df_book['FreebasePredicate'] == 'www.freebase.com/book/written_work/original_language'] = 'language'\n",
    "simple_df_book['FreebasePredicate'].loc[simple_df_book['FreebasePredicate'] == 'www.freebase.com/cvg/computer_videogame/publisher'] = 'publisher'\n",
    "\n",
    "simple_df_book['FreebasePredicate'].loc[simple_df_book['FreebasePredicate'] == 'www.freebase.com/film/film/country'] = 'country'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simple_df_book)/len(simple_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df_book['FreebasePredicate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution plot of filtered dataset\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = simple_df_book['FreebasePredicate'].value_counts().sort_index(ascending=True)\n",
    "ax.plot(kind='bar')\n",
    "fig.savefig('images/data_book.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question length for subset dataset -> change to complete dataset\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.hist([len(question.split()) for question in simple_df_book['Query']], bins= 15)\n",
    "fig.savefig('images/len_question.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(question.split()) for question in simple_df_book['Query']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df_book.to_csv('data_outputs/book_df.csv', index=False)"
   ]
  },
  {
   "source": [
    "## Improving dataset of books by question generation of abstracts of books in DBpedia"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe from abstracts\n",
    "abstract_query = sparql.query('http://dbpedia.org/sparql', \"\"\"SELECT DISTINCT ?s ?label\n",
    "WHERE {  ?s rdf:type dbo:Book .\n",
    "  ?s dbo:abstract ?label .\n",
    "    FILTER (lang(?label) = 'en')}\n",
    "\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_list = []\n",
    "for row in abstract_query:\n",
    "    values = sparql.unpack_row(row)\n",
    "    abstract_list.append(values)\n",
    "abstract_df = pd.DataFrame(abstract_list, columns=['book', 'abstract'])\n",
    "abstract_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load question generated from abstracts\n",
    "import json\n",
    "\n",
    "with open('data_outputs/abstract.json') as f:\n",
    "    abstract = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get similarity between a target type of questions and an input question\n",
    "def make_similar(reference, input_q):\n",
    "    q_vec = []\n",
    "    input_vec = nlp(input_q)\n",
    "\n",
    "    for question in reference:\n",
    "        question_vec = nlp(question)\n",
    "        q_vec.append(input_vec.similarity(question_vec))\n",
    "        return max(q_vec)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get generated question\n",
    "question_list = [question for qg in abstract.values() for question in qg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace entities by their label\n",
    "delex_questions = []\n",
    "for question in question_list:\n",
    "    doc = nlp(question)\n",
    "    new_question = question\n",
    "    for ent in reversed(doc.ents):\n",
    "        start = ent.start_char\n",
    "        end = start + len(ent.text)\n",
    "        new_question = new_question[:start]+ ent.label_ + new_question[end:]\n",
    "    delex_questions.append(new_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(zip(question_list, delex_questions), columns = ['question', 'delex_question'])\n",
    "new_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_target = ['In date was published','in date was first published', 'when was the book published', 'when was the book released', 'when was work of art first published', 'in date was originally published', 'when was the novel published', \"what is the released year\",\"when was oublished\"]\n",
    "date_similarity = [make_similar(date_target, input_) for input_ in new_df['delex_question']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['date_similarity'] = date_similarity\n",
    "date_df = list(set(new_df['question'][new_df['date_similarity']>0.90].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "others_date = [q for q in date_df for w in q.split() if w.lower() in ['where', 'who', 'many','which', 'country', 'city']]\n",
    "date_df = [q for q in date_df if q not in others_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = pd.DataFrame(date_df, columns=['Query'])\n",
    "date_df['pred'] = 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lang = simple_df_book['Query'][simple_df_book['FreebasePredicate']=='language']\n",
    "lang_target_questions = []\n",
    "for question in simple_lang:\n",
    "    doc = nlp(question)\n",
    "    new_question = question\n",
    "    for ent in reversed(doc.ents):\n",
    "        start = ent.start_char\n",
    "        end = start + len(ent.text)\n",
    "        new_question = new_question[:start]+ ent.label_ + new_question[end:]\n",
    "    lang_target_questions.append(new_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lang_question = simple_df_book['Query'][simple_df_book['FreebasePredicate']=='language'].to_list()\n",
    "lang_question = [ \"what language was book \", \"in which language did the book appear\",\"what is the original language of the book\", \"in what language was realeased\", \"in what language was originally published\", \"what language is book written\", \"what language was book originally?\",\"what language was the original edition?\",\"what language was the novel written in?\"]\n",
    "new_df['lang_similarity'] = [make_similar(lang_question, input_) for input_ in new_df['delex_question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_list = new_df['question'][new_df['lang_similarity']>0.90].to_list()\n",
    "others = [q for q in lang_list for w in q.split() if w.lower() in ['where', 'who', 'how','many', 'when', 'country']]\n",
    "lang_df = [q for q in lang_list if q not in others]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_df = lang_df['Query'].to_list()[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_df = pd.DataFrame(lang_df, columns=['Query'])\n",
    "lang_df['pred'] = 'language'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = pd.DataFrame(simple_df['Query'][simple_df['FreebasePredicate'] == 'www.freebase.com/film/film/country'])\n",
    "country_df['pred'] = 'country'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.hist([make_similar(date_questions, input_) for input_ in country_df['Query']], bins= 15)\n",
    "fig.savefig('images/country.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_simple_df = simple_df_book[['Query', 'FreebasePredicate']]\n",
    "intent_simple_df.columns = ['Query', 'pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([intent_simple_df,  date_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_simple_df['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution plot of filtered dataset -> change to complete dataset\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = final_df['pred'].value_counts().sort_index(ascending=True)\n",
    "ax.plot(kind='bar')\n",
    "fig.savefig('images/data_book_final.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([len(question) for question in final_df['Query']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question length for subset dataset -> change to complete dataset\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.hist([len(question.split()) for question in final_df['Query']], bins= 50)\n",
    "fig.savefig('len_question_book.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('data_outputs/new_intent_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_simple_df.to_csv('data_outputs/simple_book_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_simple_df['pred'].value_counts()"
   ]
  },
  {
   "source": [
    "# NEL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NEL_spotlight(question, support):\n",
    "\n",
    "    question_annotations = []\n",
    "    try: \n",
    "        annotations = spotlight.annotate('https://api.dbpedia-spotlight.org/en/annotate', question, confidence=0.4, support=20) \n",
    "        question_annotations.append(annotations[0]['URI']) \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return question_annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ram-g-athreya/RNN-Question-Answering/blob/master/slot_filling.ipynb\n",
    "def tagme_annotation(token, question):\n",
    "    ann_list = []\n",
    "    response = requests.get(\"https://tagme.d4science.org/tagme/tag?lang=en&gcube-token={}&text={}\".format(token, question))\n",
    "\n",
    "    annotations = {}\n",
    "    if response.status_code == 200 :\n",
    "      for annotation in json.loads(response.text)['annotations']: \n",
    "             \n",
    "        annotations[('http://dbpedia.org/resource/' + annotation['title'].replace(' ', '_'))] = annotation['rho']\n",
    "    else: \n",
    "      annotations.append('')\n",
    "\n",
    "    return sorted(annotations.items(), key=lambda x: x[1])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nel_df = pd.read_csv('data_outputs/nel_data.csv')\n",
    "nel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nel_df['spot0.3'] = nel_df['spot0.3'].replace(np.nan, '', regex=True)\n",
    "nel_df['spot0.4'] = nel_df['spot0.4'].replace(np.nan, '', regex=True)\n",
    "nel_df['spot0.5'] = nel_df['spot0.5'].replace(np.nan, '', regex=True)\n",
    "nel_df['spot0.6'] = nel_df['spot0.6'].replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nel_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def qa_pred (data, pred):\n",
    "    true = []\n",
    "    not_ent = []\n",
    "    for i in range(len(data)):\n",
    "        if pred[i] == '':\n",
    "            true.append(0.0)\n",
    "            not_ent.append(1.0)\n",
    "        elif pred[i] == data[i]:\n",
    "            true.append(1.0)\n",
    "        else:\n",
    "            true.append(0.0)\n",
    "\n",
    "    fp = len(data)-sum(not_ent)-sum(true)\n",
    "    fn = len(data)-sum(true)\n",
    "    recall = sum(true)/(sum(true)+fn)\n",
    "    precision = sum(true)/(sum(true)+fp)\n",
    "    f1 = 2*((precision*recall)/(precision+recall))\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pred(nel_df['Subject'], nel_df['spot0.3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pred(nel_df['Subject'], nel_df['tagme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1,ax2), (ax3, ax4))= plt.subplots(nrows=2, ncols=2, figsize=(12,5))\n",
    "ax1.hist(nel_df['tagme_rho'][nel_df['spot0.3']== '' ], bins= 15)\n",
    "ax1.set_title('Tagme goodness DBpedia Spotlight confidence of 0.3')\n",
    "ax2.hist(nel_df['tagme_rho'][nel_df['spot0.4']== '' ], bins= 15)\n",
    "ax2.set_title('Tagme goodness DBpedia Spotlight confidence of 0.4')\n",
    "ax3.hist(nel_df['tagme_rho'][nel_df['spot0.5']== '' ], bins= 15)\n",
    "ax3.set_title('Tagme goodness vs DBpedia Spotlight confidence of 0.5')\n",
    "ax4.hist(nel_df['tagme_rho'][nel_df['spot0.6']== '' ], bins= 15)\n",
    "ax4.set_title('Tagme goodness DBpedia Spotlight confidence of 0.6')\n",
    "plt.tight_layout()\n",
    "git st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1,1, figsize=(10,5))\n",
    "ax.hist(nel_df['tagme_rho'][nel_df['spot0.3']== '' ], bins= 15)\n",
    "ax.set_title('Tagme goodness for DBpedia Spotlight annotations withc a confidence score of 0.3')\n",
    "fig.savefig('images/dbpedia.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nel_df[nel_df['spot0.6']=='']"
   ]
  },
  {
   "source": [
    "# Intent recognition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('data_outputs/final_intent_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries for BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "max_len = max([len(question.split()) for question in final_df['Query']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sequence embedding with BERT\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for question in final_df['Query']:\n",
    "  enc_question = tokenizer.encode_plus(question, add_special_tokens= True, max_length=max_len, pad_to_max_length = True, return_attention_mask = True)\n",
    "  input_ids.append(enc_question['input_ids'])\n",
    "  attention_masks.append(enc_question['attention_mask'])\n",
    "\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "intent_val = encoder.fit_transform(final_df['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(intent_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data in train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test, train_mask, val_mask = train_test_split(input_ids , intent_val, attention_masks , random_state=42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size +1 \n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model without class weights\n",
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_len),\n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation = 'softmax')], name = 'LSTM')\n",
    "  \n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(lr=2e-4), metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,epochs = 10, validation_data=(x_test, y_test), verbose = 1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with class weights\n",
    "import tensorflow as tf\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_len),\n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation = 'softmax')], name = 'LSTM')\n",
    "  \n",
    "\n",
    "lstm_model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(lr=2e-4), metrics=['acc'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = list(class_weight.compute_class_weight('balanced', np.unique(final_df['pred']), final_df['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "for idx, weight in enumerate(class_weight):\n",
    "    weights[idx] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_weights = lstm_model.fit(x_train, y_train,epochs = 10, validation_data=(x_test, y_test), verbose = 1, batch_size = 8, class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "ax1.plot(history.history['acc'])\n",
    "ax1.plot(history.history['val_acc'])\n",
    "ax1.set_title('model accuracy for LSTM without weights')\n",
    "ax1.legend(['train','val'], loc='right')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_xlabel('epoch')\n",
    "\n",
    "ax2.plot(history_weights.history['acc'])\n",
    "ax2.plot(history_weights.history['val_acc'])\n",
    "ax2.set_title('model accuracy for LSTM with class weights')\n",
    "ax2.legend(['train','val'], loc='right')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.set_xlabel('epoch')\n",
    "fig.savefig('images/lstm_accuracy.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "ax1.set_title('Loss for LSTM without weights')\n",
    "ax1.legend(['train','val'], loc='right')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "\n",
    "ax2.plot(history_weights.history['loss'])\n",
    "ax2.plot(history_weights.history['val_loss'])\n",
    "ax2.set_title('Loss for LSTM with class weights')\n",
    "ax2.legend(['train','val'], loc='right')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "fig.savefig('images/lstm_loss.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction of test cases\n",
    "from sklearn.metrics import classification_report\n",
    "preds = model.predict_classes(x_test)\n",
    "print(classification_report(y_test, preds,target_names = encoder.inverse_transform([i for i in range(8)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds = lstm_model.predict_classes(x_test)\n",
    "print(classification_report(y_test, preds,target_names = encoder.inverse_transform([i for i in range(8)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "question_test  = \"which type of book is alice in wonderland \"\n",
    "q_test_enc = tokenizer.encode(question_test, add_special_tokens= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = lstm_model.predict_classes(q_test_enc)\n",
    "prediction_ = np.argmax(preds, axis=-1)\n",
    "encoder.inverse_transform([prediction_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save('models/bilstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_model = tf.keras.models.load_model('bilstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  question = \"what is the language of Moby dick\"\n",
    "  enc_question = tokenizer.encode_plus(question, add_special_tokens= True, max_length=24, pad_to_max_length = True, return_attention_mask = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bilstm_model.predict(enc_question)\n",
    "\n",
    "prediction_ = np.argmax(preds, axis=-1)\n",
    "encoder.inverse_transform([0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}